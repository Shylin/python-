#### 文本预处理流程

##### 1.分词（将自然语言按语义进行分隔，得到向量化的列表）

```
# 英文分词
import nltk
text = "Python is a high-level programming language, and I like it!"
# 将文本按语义分词，返回列表
seg_list = nltk.word_tokenize(text)
print(seg_list)
# ['Python', 'is', 'a', 'high-level', 'programming', 'language', ',', 'and', 'I', 'like', 'it', '!']

# 中文分词
import jieba
text = "欢迎来到召唤师峡谷"
seg_list1 = jieba.cut(text) # 默认cut_all=False 精确模式：按中文语义精确分隔文本（语义分析）
print(list(seg_list1)) # ['欢迎', '来到', '召唤师', '峡谷']
seg_list = jieba.cut(text, cut_all=True) # 全模式：将所有可能成词的结果全部分隔出来 （词频统计）
print(list(seg_list)) # ['欢迎', '迎来', '来到', '召唤', '召唤师', '峡谷']
seg_list = jieba.cut_for_search(text) # 搜索引擎模式：在精确模式的基础上，对长词进行二次分隔，提高每个词的使用率（站内搜索）
print(list(seg_list)) # ['欢迎', '来到', '召唤', '召唤师', '峡谷']
```

##### 2.词形处理（用于英文）

```
#2.1 词干提取 ：去除单词后缀，留下主干部分 look/looking/looked
#2.2 词形归并：将单词按统一的词性进行处理
```

##### 3.词性标注：对每个单词标注词性的过程（可选）

```
# 英文词性标注
import nltk
text = "Python is a high-level programming language, and I like it!"
# 分词
seg_list = nltk.word_tokenize(text)
# 标注词性
pos_list = nltk.pos_tag(seg_list)
print(pos_list) # [('Python', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('high-level', 'JJ'), ('programming', 'NN'),...]

# 中文词性标注
text = "欢迎来到召唤师峡谷"
# 分词的同时并标注词性
seg_list = jieba.posseg.cut(text)
for word, pos in seg_list:
    print(word, pos)
欢迎 v
来到 v
召唤师 i
峡谷 n
```

#####  4. 去停用词（文本中大量出现但没有具体含义的词，避免影响统计分析结果，需要去除）

```
from nltk.corpus import stopwords
text = "Python is a high-level programming language, and I like it!"
# 先获取停用词库
stopword_list = stopwords.words("english")
# 再添加新的停用词
stopword_list.append("like")
stopword_list.append(",")
stopword_list.append("!")
## 1. 分词
seg_list = nltk.word_tokenize(text)
## 2. 去停用词 : 判断每个单词是否存在于停用词库，如果不存在则返回该词并保存到新列表中（英文处理注意转小写判断）
word_list = [seg for seg in seg_list if seg.lower() not in stopword_list]
print(word_list)
# ['Python', 'high-level', 'programming', 'language']
```

